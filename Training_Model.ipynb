{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nba_api\n",
    "import time\n",
    "import ast\n",
    "from nba_api.stats.static import teams\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filename, start_date, end_date):\n",
    "    nba_teams = teams.get_teams()\n",
    "    # if the file does not exist, create with a csv that aggregates all raw data\n",
    "    if (path.exists(filename) != True):\n",
    "        \n",
    "        team_id = nba_teams[0]['id']\n",
    "        gamefinder = leaguegamefinder.LeagueGameFinder(date_from_nullable = start_date , date_to_nullable = end_date , team_id_nullable = team_id)\n",
    "        games = gamefinder.get_data_frames()[0]\n",
    "        games.to_csv(filename,index=False)\n",
    "    \n",
    "    # if the file exists, start with the row that the csv left off at\n",
    "    if (path.exists(filename) == True):   \n",
    "        \n",
    "        old_df = pd.read_csv(filename)\n",
    "        last_id = old_df['TEAM_ID'][len(old_df)-1]\n",
    "        start_id = int(last_id) + 1\n",
    "        \n",
    "        while start_id <= 1610612766:\n",
    "            old_df = pd.read_csv(filename)\n",
    "            gamefinder = leaguegamefinder.LeagueGameFinder(date_from_nullable = start_date , date_to_nullable = end_date , team_id_nullable = start_id)\n",
    "            games = gamefinder.get_data_frames()[0]\n",
    "            new_df = old_df.append(games)\n",
    "            new_df.to_csv(filename, index=False)\n",
    "            start_id = start_id + 1\n",
    "        off_reb_given_up(filename)\n",
    "        days_rest(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_reb_given_up(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    off_reb_given_up = []\n",
    "    for row in df.iterrows():\n",
    "        catch = df.loc[df['GAME_ID'] == row[1]['GAME_ID']]\n",
    "        row[1]['OREB_GIVEN_UP'] = catch.iloc[1]['OREB']\n",
    "        off_reb_given_up.append(row[1]['OREB_GIVEN_UP'])\n",
    "    df.insert(loc=(df.columns.get_loc(\"REB\")) + 1, column=\"OREB_GIVEN_UP\", value=off_reb_given_up)\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_rest(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_id = nba_teams[0]['id']\n",
    "    df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "\n",
    "    df['REST'] = df.groupby(['TEAM_ABBREVIATION'])['GAME_DATE'].diff()\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_handle(count, filename, start_date, end_date):\n",
    "    try:\n",
    "        print(\"-----try is running-----\")\n",
    "        # put csv name here\n",
    "        getData(filename, start_date, end_date)\n",
    "        count = 0\n",
    "        \n",
    "    except:\n",
    "        if count < 25:\n",
    "            print(\"-----exception handled-----\", count)\n",
    "            error_handle(count + 1,filename, start_date, end_date)\n",
    "        else:\n",
    "            print(\"-----max tries exceeded-----\")\n",
    "    \n",
    "    nba_teams = teams.get_teams()\n",
    "    csv_df = pd.read_csv(filename)\n",
    "    cdf = csv_df.sort_values(['TEAM_ABBREVIATION','GAME_DATE'] , ascending=[True, True])\n",
    "    cdf.to_csv(filename, index=False)\n",
    "    \n",
    "    #return rolling_average_stats(filename, 'ten_day-' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of csv to read, name of csv to write\n",
    "def rolling_average_stats(r_filename, w_filename, i):  \n",
    "    print('Inside rolling_average_stats')\n",
    "    \n",
    "    nba_teams = teams.get_teams()\n",
    "    csv_df = pd.read_csv(r_filename)\n",
    "\n",
    "    list_points = []\n",
    "    list_team_points = []\n",
    "    x = 1\n",
    "    # for each team in csv, calculate the rolling average based on the parameter, i, that is passed in\n",
    "    for team in nba_teams:\n",
    "        team_df = csv_df[csv_df['TEAM_ID'] == team['id']]\n",
    "        for col in team_df.columns[9:]:\n",
    "            team_df['AV_'+ col] = team_df[col].rolling(window=i).mean()\n",
    "            team_df['AV_'+ col] = team_df['AV_'+ col].shift(1)\n",
    "        head = list(team_df.columns.values)\n",
    "        if x == 1:\n",
    "            team_df.to_csv(w_filename, header=head, index=False)\n",
    "            x = x+1\n",
    "\n",
    "        else:\n",
    "            team_df.to_csv(w_filename, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_team_games(df, keep_method='home'):\n",
    "    '''Combine a TEAM_ID-GAME_ID unique table into rows by game. Slow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : Input DataFrame.\n",
    "        keep_method : {'home', 'away', 'winner', 'loser', ``None``}, default 'home'\n",
    "            - 'home' : Keep rows where TEAM_A is the home team.\n",
    "            - 'away' : Keep rows where TEAM_A is the away team.\n",
    "            - 'winner' : Keep rows where TEAM_A is the losing team.\n",
    "            - 'loser' : Keep rows where TEAM_A is the winning team.\n",
    "            - ``None`` : Keep all rows. Will result in an output DataFrame the same\n",
    "                length as the input DataFrame.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        result : DataFrame\n",
    "    '''\n",
    "    # Join every row to all others with the same game ID.\n",
    "    joined = pd.merge(df, df, suffixes=['_A', '_B'],\n",
    "                      on=['SEASON_ID', 'GAME_ID', 'GAME_DATE'])\n",
    "    # Filter out any row that is joined to itself.\n",
    "    result = joined[joined.TEAM_ID_A != joined.TEAM_ID_B]\n",
    "    # Take action based on the keep_method flag.\n",
    "    if keep_method is None:\n",
    "        # Return all the rows.\n",
    "        pass\n",
    "    elif keep_method.lower() == 'home':\n",
    "        # Keep rows where TEAM_A is the home team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' vs. ')]\n",
    "    elif keep_method.lower() == 'away':\n",
    "        # Keep rows where TEAM_A is the away team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' @ ')]\n",
    "    elif keep_method.lower() == 'winner':\n",
    "        result = result[result.WL_A == 'W']\n",
    "    elif keep_method.lower() == 'loser':\n",
    "        result = result[result.WL_A == 'L']\n",
    "    else:\n",
    "        raise ValueError(f'Invalid keep_method: {keep_method}')\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses combine function and cleans csv\n",
    "# ten_day_csv is the csv with the rolling ten day averages for a year\n",
    "# combined_csv is the returned csv with teams combined with their matchups\n",
    "def combine_and_clean(ten_day_csv, combined_csv):\n",
    "    print('Inside combine_and_clean')\n",
    "    \n",
    "    attempt = pd.read_csv(ten_day_csv,index_col=[0])\n",
    "    # Drop these fields to only leave averages\n",
    "    attempt = attempt.drop(['PTS','FGM','FGA','FG_PCT','FG3M','FG3A','FG3_PCT','FTM','FTA','FT_PCT','OREB','DREB','REB','AST','STL','BLK','TOV','PF', 'PLUS_MINUS'],axis=1)\n",
    "\n",
    "    count = 0\n",
    "    # combine rows from rolling average pdf so that it illustrates TEAM_A attributes vs TEAM_B attributes (side by side)\n",
    "    for row in attempt.iterrows():\n",
    "        # creates csv\n",
    "        if (count == 0):\n",
    "            catch = attempt.loc[attempt['GAME_ID'] == row[1]['GAME_ID']]\n",
    "            catch = pd.DataFrame(catch)\n",
    "            combine = combine_team_games(catch)\n",
    "            combine.to_csv(combined_csv, index=False)\n",
    "            count = count + 1\n",
    "        # appends csv\n",
    "        else: \n",
    "            old_df = pd.read_csv(combined_csv)\n",
    "            catch = attempt.loc[attempt['GAME_ID'] == row[1]['GAME_ID']]\n",
    "            catch = pd.DataFrame(catch)\n",
    "            combine = combine_team_games(catch)\n",
    "            new_df = old_df.append(combine)\n",
    "            new_df.to_csv(combined_csv, index=False)\n",
    "    \n",
    "    clean = pd.read_csv(combined_csv)\n",
    "    # drops duplicates, sort by game date, and replace W with 1 and L with 0\n",
    "    cleaned = clean.drop_duplicates(subset='GAME_ID')\n",
    "    cleaned = cleaned.sort_values('GAME_DATE')\n",
    "    cleaned['WL_A'] = cleaned['WL_A'].replace(['W','L'],[1,0])\n",
    "    cleaned['WL_B'] = cleaned['WL_B'].replace(['W','L'],[1,0])\n",
    "    cleaned.to_csv(combined_csv, index=False)\n",
    "    return combined_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscore_for_one_year(cleaned_csv):\n",
    "    #print(\"Inside get_zscore_for_one_year\")\n",
    "    \n",
    "    data = pd.read_csv(cleaned_csv)\n",
    "    data = data.dropna()\n",
    "\n",
    "    z_data = pd.DataFrame(columns = ['GAME_ID', 'GAME_DATE', 'MATCHUP','WL', 'PTS', 'FGM' , 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PLUS_MINUS'])\n",
    "    z_data['WL'] = data['WL_A']\n",
    "    z_data['GAME_ID'] = data['GAME_ID']\n",
    "    z_data['GAME_DATE'] = data['GAME_DATE']\n",
    "    z_data['MATCHUP'] = data['MATCHUP_A']\n",
    "    # for each of the rolling averages that were calculated, take the difference of TEAM_A's averages - TEAM_B's averages\n",
    "    for column in z_data.columns[4:]:\n",
    "        z_data[column] = data['AV_' + column + '_A'] - data['AV_' + column + '_B']\n",
    "    # Formatting\n",
    "    z_data = z_data.dropna()\n",
    "    z_data = z_data.round(decimals=3)\n",
    "    return z_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscores(year1_cleaned_csv, year2_cleaned_csv, year3_cleaned_csv):\n",
    "    \n",
    "    df1 = get_zscore_for_one_year(year1_cleaned_csv)\n",
    "    df2 = get_zscore_for_one_year(year2_cleaned_csv)\n",
    "    df3 = get_zscore_for_one_year(year3_cleaned_csv)\n",
    "    \n",
    "    df1 = df1.append(df2)\n",
    "    df1 = df1.append(df3)\n",
    "    df1.to_csv(\"all_zscores.csv\", index=False)\n",
    "    return performLogReg(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the logistic regression model and tests accuracy\n",
    "def performLogReg(dataframe):\n",
    "\n",
    "    # Update if new stats are added\n",
    "    featureColumns = ['PTS', 'FGM', 'FGA', 'FG3_PCT', 'FTA','REB', 'AST',  'STL', 'TOV']\n",
    "\n",
    "    X = dataframe[featureColumns] # Features\n",
    "    Y = dataframe['WL'] # Target Variable\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=True)\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    logreg.fit(X_train, Y_train)  # Fits model with data\n",
    "    filename = 'finalized_model.sav'\n",
    "    pickle.dump(logreg, open(filename, 'wb'))\n",
    "\n",
    "    Y_pred = logreg.predict(X_test)\n",
    "\n",
    "    confusionMatrix = metrics.confusion_matrix(Y_test, Y_pred)  # Diagonals tell you correct predictions\n",
    "\n",
    "    # Code below prints model accuracy information\n",
    "    print('Coefficient Information:')\n",
    "\n",
    "    for i in range(len(featureColumns)):  # Prints each feature next to its corresponding coefficient in the model\n",
    "\n",
    "        logregCoefficients = logreg.coef_\n",
    "\n",
    "        currentFeature = featureColumns[i]\n",
    "        currentCoefficient = logregCoefficients[0][i]\n",
    "\n",
    "        print(currentFeature + ': ' + str(currentCoefficient))\n",
    "\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(Y_test, Y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(Y_test, Y_pred))\n",
    "\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusionMatrix)\n",
    "\n",
    "    return logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_zscore(cleaned_csv):\n",
    "    data = pd.read_csv(cleaned_csv)\n",
    "\n",
    "    #### Organize matchup and find index of home team####\n",
    "    matchup_list = [ast.literal_eval(i) for i in data['MATCHUP']]\n",
    "    idx_ = [(j, val) for i in matchup_list for j, val in enumerate(i) if 'vs' in val]\n",
    "    ## if idx is 0 subtract like normal other wise reverse and subtract\n",
    "    zscore_list_ = []\n",
    "    for i, index_row in zip(idx_ ,data.iterrows()):\n",
    "        temp = []\n",
    "        temp.append(index_row[1][0])\n",
    "        row = []\n",
    "        for j,k in enumerate(list(index_row[1])):\n",
    "            if j != 0:\n",
    "                if 'nan' in k:\n",
    "                    temp.append(\"None\")\n",
    "                else:\n",
    "                    temp.append(ast.literal_eval((k)))\n",
    "        if i[0] == 0:\n",
    "            #home index is '0'\n",
    "            row.append(temp[0])#game_id\n",
    "            row.append(temp[5][0]) #date\n",
    "            row.append(temp[6][0]) #matchup\n",
    "            w_l = 1 if temp[7][0] == 'W' else 0\n",
    "            row.append(w_l)\n",
    "            row.extend(j[0] - j[1]  if (j) != 'None' else 'None' for j in temp[28:])\n",
    "        else:\n",
    "            #home index is '1'\n",
    "            row.append(temp[0])#game_id\n",
    "            row.append(temp[5][0]) #date\n",
    "            row.append(temp[6][1]) #matchup\n",
    "            w_l = 1 if temp[7][1] == 'W' else 0\n",
    "            row.append(w_l)\n",
    "            row.extend(j[1] - j[0]  if (j ) != 'None' else 'None' for j in temp[28:])\n",
    "        zscore_list_.append(row)    \n",
    "    df = pd.DataFrame(zscore_list_, columns = ['GAME_ID', 'GAME_DATE', 'MATCHUP','WL', 'PTS', 'FGM' , 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PLUS_MINUS'])\n",
    "    df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "    return df.round(decimals=3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes name of rolling csv and new name\n",
    "def combine_and_clean_fast(rolling, combined_csv):\n",
    "    df = pd.read_csv(rolling)\n",
    "    gid_by_team = df.groupby('GAME_ID').agg(lambda x : list(x) )\n",
    "    gid_by_team.to_csv(combined_csv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_date_dict = {\n",
    "    \"17-18\" : [\"10/17/2017\", \"06/17/2018\"],\n",
    "    \"18-19\" : [\"10/16/2018\", \"06/13/2019\"],\n",
    "    \"19-20\" : [\"10/22/2019\", \"10/11/2020\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_system(year = \"19-20\", rolling = [10]):\n",
    "    \"\"\" \n",
    "    params:\n",
    "    year | specify season e.g. \"19-20\" or range e.g. \"17-20\"\n",
    "    rolling | specify amount of days as list e.g. \"[3,10] or [10]\"\n",
    "    \"\"\"\n",
    "    #get raw data for years specified\n",
    "    range_years = year.split(\"-\")\n",
    "    if (int(range_years[0]) - int(range_years[1])) != 1:\n",
    "        x = range_years[0]\n",
    "        years_list = []\n",
    "        while int(x) < int(range_years[1]):\n",
    "            y = int(x)+1\n",
    "            years_list.append(x + \"-\" + str(y))\n",
    "            x = str(y)\n",
    "        outdir = './Raw_Data'\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        for y in years_list:\n",
    "            fname = \"20\" + y + \".csv\"\n",
    "            fullname = os.path.join(outdir, fname)  \n",
    "            error_handle(0, fullname, season_date_dict[y][0], season_date_dict[y][1])\n",
    "    else:\n",
    "        outdir = './Raw_Data'\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        fname = \"20\" + year + \".csv\"\n",
    "        fullname = os.path.join(outdir, fname)  \n",
    "        error_handle(0, fullname, season_date_dict[year][0], season_date_dict[year][1])\n",
    "\n",
    "    #adjust rolling average deadline\n",
    "    if type(rolling == list):\n",
    "        xdir = './Rolling_Averages'\n",
    "        if not os.path.exists(xdir):\n",
    "            os.mkdir(xdir)\n",
    "        files = [ f for f in listdir('./Raw_Data') if isfile(join('./Raw_Data', f))]      \n",
    "\n",
    "        for f in files:            \n",
    "            for i in rolling:\n",
    "                wname =  str(i)+ \"_rolling_\" + f\n",
    "                w_filename = os.path.join(xdir, wname) \n",
    "                rolling_average_stats('./Raw_Data/' + f, w_filename, i)\n",
    "                \n",
    "#     combine and clean team data\n",
    "    cdir = './Combined_Data'\n",
    "    if not os.path.exists(cdir):\n",
    "        os.mkdir(cdir)\n",
    "    r_files = [ f for f in listdir('./Rolling_Averages') if isfile(join('./Rolling_Averages', f))]      \n",
    "    for f in r_files:\n",
    "        combine_and_clean_fast('./Rolling_Averages/'+ f, './Combined_Data/'+'c_' + f )\n",
    "\n",
    "    #zscore and finalize\n",
    "    zdir = './Zscores'\n",
    "    if not os.path.exists(zdir):\n",
    "        os.mkdir(zdir)\n",
    "        \n",
    "    z_files = [ f for f in listdir('./Combined_Data') if isfile(join('./Combined_Data', f))]\n",
    "   \n",
    "    z_index = []\n",
    "    for f in z_files:\n",
    "        split = f.split('_')\n",
    "        i = (split[1])\n",
    "        if i not in z_index:\n",
    "            z_index.append(i)\n",
    "    z_2d_files = [[f for f in z_files if f[2:(2+len(i))]==i] for i in z_index ]\n",
    "    \n",
    "    for i in z_2d_files:\n",
    "        #z_score_df = pd.DataFrame()\n",
    "        #make zscores \n",
    "        for j in i:\n",
    "            df = fast_zscore('./Combined_Data/'+j)\n",
    "            df = df.sort_values('GAME_DATE')\n",
    "            df.to_csv(zdir + \"/z_\" + j[2:], index=False)  \n",
    "           \n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = big_system(year = \"17-20\", rolling = [3,5,10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside combine_and_clean\n",
      "303.982174988\n",
      "Inside combine_and_clean\n",
      "234.12189158599995\n",
      "Inside combine_and_clean\n",
      "228.5986795670001\n",
      "Inside combine_and_clean\n",
      "219.07340677599996\n",
      "Inside combine_and_clean\n",
      "232.92488859900004\n",
      "Inside combine_and_clean\n",
      "228.87509998799987\n",
      "Inside combine_and_clean\n",
      "216.29968133399984\n",
      "Inside combine_and_clean\n",
      "363.20393629\n",
      "Inside combine_and_clean\n",
      "290.897709759\n"
     ]
    }
   ],
   "source": [
    "# cdir = './Combined_Data'\n",
    "# if not os.path.exists(cdir):\n",
    "#     os.mkdir(cdir)\n",
    "# r_files = [ f for f in listdir('./Rolling_Averages') if isfile(join('./Rolling_Averages', f))]      \n",
    "# for f in r_files:\n",
    "#     before_load = time.perf_counter()\n",
    "#     combine_and_clean('./Rolling_Averages/'+ f, './Combined_Data/'+'c_' + f )\n",
    "#     after_load = time.perf_counter()\n",
    "#     print(after_load - before_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-2419848a1521>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-2419848a1521>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    gid_by_team = df.groupby('GAME_ID').agg(lambda x: print(x) break)\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_rolling_2018-19.csv\n",
      "3.5105376859992248\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = fast_zscore('test3.csv')\n",
    "# df.to_csv('z_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
