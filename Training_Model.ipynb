{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nba_api\n",
    "\n",
    "from nba_api.stats.static import teams\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filename, start_date, end_date):\n",
    "    nba_teams = teams.get_teams()\n",
    "    if (path.exists(filename) != True):\n",
    "        \n",
    "        team_id = nba_teams[0]['id']\n",
    "        gamefinder = leaguegamefinder.LeagueGameFinder(date_from_nullable = start_date , date_to_nullable = end_date , team_id_nullable = team_id)\n",
    "        games = gamefinder.get_data_frames()[0]\n",
    "        games.to_csv(filename,index=False)\n",
    "        \n",
    "    if (path.exists(filename) == True):   \n",
    "        \n",
    "        old_df = pd.read_csv(filename)\n",
    "        last_id = old_df['TEAM_ID'][len(old_df)-1]\n",
    "        start_id = int(last_id) + 1\n",
    "        \n",
    "        while start_id <= 1610612766:\n",
    "            old_df = pd.read_csv(filename)\n",
    "            gamefinder = leaguegamefinder.LeagueGameFinder(date_from_nullable = start_date , date_to_nullable = end_date , team_id_nullable = start_id)\n",
    "            games = gamefinder.get_data_frames()[0]\n",
    "            new_df = old_df.append(games)\n",
    "            new_df.to_csv(filename, index=False)\n",
    "            start_id = start_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_handle(count, filename, start_date, end_date):\n",
    "    try:\n",
    "        print(\"-----try is running-----\")\n",
    "        # put csv name here\n",
    "        getData(filename, start_date, end_date)\n",
    "        count = 0\n",
    "        \n",
    "    except:\n",
    "        if count < 25:\n",
    "            print(\"-----exception handled-----\", count)\n",
    "            error_handle(count + 1,filename, start_date, end_date)\n",
    "        else:\n",
    "            print(\"-----max tries exceeded-----\")\n",
    "    \n",
    "    nba_teams = teams.get_teams()\n",
    "    csv_df = pd.read_csv(filename)\n",
    "    cdf = csv_df.sort_values(['TEAM_ABBREVIATION','GAME_DATE'] , ascending=[True, True])\n",
    "    cdf.to_csv(filename, index=False)\n",
    "    \n",
    "    #return rolling_average_stats(filename, 'ten_day-' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of csv to read, name of csv to write\n",
    "def rolling_average_stats(r_filename, w_filename, i):  \n",
    "    print('Inside rolling_average_stats')\n",
    "    \n",
    "    nba_teams = teams.get_teams()\n",
    "    csv_df = pd.read_csv(r_filename)\n",
    "\n",
    "    list_points = []\n",
    "    list_team_points = []\n",
    "    x = 1\n",
    "    for team in nba_teams:\n",
    "        team_df = csv_df[csv_df['TEAM_ID'] == team['id']]\n",
    "        for col in team_df.columns[9:]:\n",
    "            team_df['AV_'+ col] = team_df[col].rolling(window=i).mean()\n",
    "            team_df['AV_'+ col] = team_df['AV_'+ col].shift(1) #add code here\n",
    "        head = list(team_df.columns.values)\n",
    "        if x == 1:\n",
    "#             new_df = team_df\n",
    "            team_df.to_csv(w_filename, header=head, index=False)\n",
    "            x = x+1\n",
    "\n",
    "        else:\n",
    "#             new_df.append(team_df)\n",
    "            team_df.to_csv(w_filename, mode='a', header=False, index=False)\n",
    "    #combine_and_clean(w_filename, \"combined-\" + w_filename)\n",
    "    #return (\"combined-\" + w_filename)\n",
    "#     z_data = get_zscore_for_one_year(combine_and_clean(w_filename, \"combined-\" + w_filename))\n",
    "#     z_data.to_csv(\"z_data_for_\" + \"combined-\" + w_filename)\n",
    "#     return \"z_data_for_\" + \"combined-\" + w_filename\n",
    "#     z_data.to_csv(\"z_data_for_\" + \"combined-\" + w_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_team_games(df, keep_method='home'):\n",
    "    '''Combine a TEAM_ID-GAME_ID unique table into rows by game. Slow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : Input DataFrame.\n",
    "        keep_method : {'home', 'away', 'winner', 'loser', ``None``}, default 'home'\n",
    "            - 'home' : Keep rows where TEAM_A is the home team.\n",
    "            - 'away' : Keep rows where TEAM_A is the away team.\n",
    "            - 'winner' : Keep rows where TEAM_A is the losing team.\n",
    "            - 'loser' : Keep rows where TEAM_A is the winning team.\n",
    "            - ``None`` : Keep all rows. Will result in an output DataFrame the same\n",
    "                length as the input DataFrame.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        result : DataFrame\n",
    "    '''\n",
    "    # Join every row to all others with the same game ID.\n",
    "    joined = pd.merge(df, df, suffixes=['_A', '_B'],\n",
    "                      on=['SEASON_ID', 'GAME_ID', 'GAME_DATE'])\n",
    "    # Filter out any row that is joined to itself.\n",
    "    result = joined[joined.TEAM_ID_A != joined.TEAM_ID_B]\n",
    "    # Take action based on the keep_method flag.\n",
    "    if keep_method is None:\n",
    "        # Return all the rows.\n",
    "        pass\n",
    "    elif keep_method.lower() == 'home':\n",
    "        # Keep rows where TEAM_A is the home team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' vs. ')]\n",
    "    elif keep_method.lower() == 'away':\n",
    "        # Keep rows where TEAM_A is the away team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' @ ')]\n",
    "    elif keep_method.lower() == 'winner':\n",
    "        result = result[result.WL_A == 'W']\n",
    "    elif keep_method.lower() == 'loser':\n",
    "        result = result[result.WL_A == 'L']\n",
    "    else:\n",
    "        raise ValueError(f'Invalid keep_method: {keep_method}')\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses combine function and cleans csv\n",
    "# ten_day_csv is the csv with the rolling ten day averages for a year\n",
    "# combined_csv is the returned csv with teams combined with their matchups\n",
    "def combine_and_clean(ten_day_csv, combined_csv):\n",
    "    print('Inside combine_and_clean')\n",
    "    \n",
    "    attempt = pd.read_csv(ten_day_csv,index_col=[0])\n",
    "    attempt = attempt.drop(['PTS','FGM','FGA','FG_PCT','FG3M','FG3A','FG3_PCT','FTM','FTA','FT_PCT','OREB','DREB','REB','AST','STL','BLK','TOV','PF', 'PLUS_MINUS'],axis=1)\n",
    "\n",
    "    count = 0\n",
    "    for row in attempt.iterrows():\n",
    "        if (count == 0):\n",
    "            catch = attempt.loc[attempt['GAME_ID'] == row[1]['GAME_ID']]\n",
    "            catch = pd.DataFrame(catch)\n",
    "            combine = combine_team_games(catch)\n",
    "            combine.to_csv(combined_csv, index=False)\n",
    "            count = count + 1\n",
    "        else: \n",
    "            old_df = pd.read_csv(combined_csv)\n",
    "            catch = attempt.loc[attempt['GAME_ID'] == row[1]['GAME_ID']]\n",
    "            catch = pd.DataFrame(catch)\n",
    "            combine = combine_team_games(catch)\n",
    "            new_df = old_df.append(combine)\n",
    "            new_df.to_csv(combined_csv, index=False)\n",
    "    \n",
    "    clean = pd.read_csv(combined_csv)\n",
    "    # drops duplicates, sort by game date, and replace W with 1 and L with 0\n",
    "    cleaned = clean.drop_duplicates(subset='GAME_ID')\n",
    "    cleaned = cleaned.sort_values('GAME_DATE')\n",
    "    cleaned['WL_A'] = cleaned['WL_A'].replace(['W','L'],[1,0])\n",
    "    cleaned['WL_B'] = cleaned['WL_B'].replace(['W','L'],[1,0])\n",
    "    cleaned.to_csv(combined_csv, index=False)\n",
    "    return combined_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscore_for_one_year(cleaned_csv):\n",
    "    #print(\"Inside get_zscore_for_one_year\")\n",
    "    \n",
    "    data = pd.read_csv(cleaned_csv)\n",
    "    data = data.dropna()\n",
    "    z_data = pd.DataFrame(columns = ['GAME_ID', 'GAME_DATE', 'MATCHUP','WL', 'PTS', 'FGM' , 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PLUS_MINUS'])\n",
    "    z_data['WL'] = data['WL_A']\n",
    "    z_data['GAME_ID'] = data['GAME_ID']\n",
    "    z_data['GAME_DATE'] = data['GAME_DATE']\n",
    "    z_data['MATCHUP'] = data['MATCHUP_A']\n",
    "    for column in z_data.columns[4:]:\n",
    "        z_data[column] = data['AV_' + column + '_A'] - data['AV_' + column + '_B']\n",
    "    z_data = z_data.dropna()\n",
    "    z_data = z_data.round(decimals=3)\n",
    "    return z_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscores(year1_cleaned_csv, year2_cleaned_csv, year3_cleaned_csv):\n",
    "    \n",
    "    df1 = get_zscore_for_one_year(year1_cleaned_csv)\n",
    "    df2 = get_zscore_for_one_year(year2_cleaned_csv)\n",
    "    df3 = get_zscore_for_one_year(year3_cleaned_csv)\n",
    "    \n",
    "    df1 = df1.append(df2)\n",
    "    df1 = df1.append(df3)\n",
    "    df1.to_csv(\"all_zscores.csv\", index=False)\n",
    "    return performLogReg(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the logistic regression model and tests accuracy\n",
    "def performLogReg(dataframe):\n",
    "\n",
    "    # Update if new stats are added\n",
    "    featureColumns = ['PTS', 'FGM', 'FGA', 'FG3_PCT', 'FTA','REB', 'AST',  'STL', 'TOV']\n",
    "\n",
    "    X = dataframe[featureColumns] # Features\n",
    "    Y = dataframe['WL'] # Target Variable\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=True)\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    logreg.fit(X_train, Y_train)  # Fits model with data\n",
    "    filename = 'finalized_model.sav'\n",
    "    pickle.dump(logreg, open(filename, 'wb'))\n",
    "\n",
    "    Y_pred = logreg.predict(X_test)\n",
    "\n",
    "    confusionMatrix = metrics.confusion_matrix(Y_test, Y_pred)  # Diagonals tell you correct predictions\n",
    "\n",
    "    # Code below prints model accuracy information\n",
    "    print('Coefficient Information:')\n",
    "\n",
    "    for i in range(len(featureColumns)):  # Prints each feature next to its corresponding coefficient in the model\n",
    "\n",
    "        logregCoefficients = logreg.coef_\n",
    "\n",
    "        currentFeature = featureColumns[i]\n",
    "        currentCoefficient = logregCoefficients[0][i]\n",
    "\n",
    "        print(currentFeature + ': ' + str(currentCoefficient))\n",
    "\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(Y_test, Y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(Y_test, Y_pred))\n",
    "\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusionMatrix)\n",
    "\n",
    "    return logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "    \n",
    "#     logreg = get_zscores(error_handle(0, \"2017-18.csv\", \"10/17/2017\", \"06/17/2018\"), error_handle(0, \"2018-19.csv\", \"10/16/2018\", \"06/13/2019\"), error_handle(0, \"2019-20.csv\", \"10/22/2019\", \"10/11/2020\"))\n",
    "#     print(logreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_date_dict = {\n",
    "    \"17-18\" : [\"10/17/2017\", \"06/17/2018\"],\n",
    "    \"18-19\" : [\"10/16/2018\", \"06/13/2019\"],\n",
    "    \"19-20\" : [\"10/22/2019\", \"10/11/2020\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_system(year = \"19-20\", rolling = [10]):\n",
    "    \"\"\" \n",
    "    params:\n",
    "    year | specify season e.g. \"19-20\" or range e.g. \"17-20\"\n",
    "    rolling | specify amount of days as list e.g. \"[3,10] or [10]\"\n",
    "    \"\"\"\n",
    "#     #get raw data for years specified\n",
    "#     range_years = year.split(\"-\")\n",
    "#     if (int(range_years[0]) - int(range_years[1])) != 1:\n",
    "#         x = range_years[0]\n",
    "#         years_list = []\n",
    "#         while int(x) < int(range_years[1]):\n",
    "#             y = int(x)+1\n",
    "#             years_list.append(x + \"-\" + str(y))\n",
    "#             x = str(y)\n",
    "#         outdir = './Raw_Data'\n",
    "#         if not os.path.exists(outdir):\n",
    "#             os.mkdir(outdir)\n",
    "#         for y in years_list:\n",
    "#             fname = \"20\" + y + \".csv\"\n",
    "#             fullname = os.path.join(outdir, fname)  \n",
    "#             error_handle(0, fullname, season_date_dict[y][0], season_date_dict[y][1])\n",
    "#     else:\n",
    "#         outdir = './Raw_Data'\n",
    "#         if not os.path.exists(outdir):\n",
    "#             os.mkdir(outdir)\n",
    "#         fname = \"20\" + year + \".csv\"\n",
    "#         fullname = os.path.join(outdir, fname)  \n",
    "#         error_handle(0, fullname, season_date_dict[year][0], season_date_dict[year][1])\n",
    "\n",
    "#     #adjust rolling average deadline\n",
    "#     if type(rolling == list):\n",
    "#         xdir = './Rolling_Averages'\n",
    "#         if not os.path.exists(xdir):\n",
    "#             os.mkdir(xdir)\n",
    "#         files = [ f for f in listdir('./Raw_Data') if isfile(join('./Raw_Data', f))]      \n",
    "\n",
    "#         for f in files:            \n",
    "#             for i in rolling:\n",
    "#                 wname =  str(i)+ \"_rolling_\" + f\n",
    "#                 w_filename = os.path.join(xdir, wname) \n",
    "#                 rolling_average_stats('./Raw_Data/' + f, w_filename, i)\n",
    "                \n",
    "    #combine and clean team data\n",
    "#     cdir = './Combined_Data'\n",
    "#     if not os.path.exists(cdir):\n",
    "#         os.mkdir(cdir)\n",
    "#     r_files = [ f for f in listdir('./Rolling_Averages') if isfile(join('./Rolling_Averages', f))]      \n",
    "#     for f in r_files:\n",
    "#         combine_and_clean('./Rolling_Averages/'+ f, './Combined_Data/'+'c_' + f )\n",
    "\n",
    "    #zscore and finalize\n",
    "    zdir = './Zscores'\n",
    "    if not os.path.exists(zdir):\n",
    "        os.mkdir(zdir)\n",
    "        \n",
    "    z_files = [ f for f in listdir('./Combined_Data') if isfile(join('./Combined_Data', f))]\n",
    "   \n",
    "    z_index = []\n",
    "    for f in z_files:\n",
    "        split = f.split('_')\n",
    "        i = (split[1])\n",
    "        if i not in z_index:\n",
    "            z_index.append(i)\n",
    "    z_2d_files = [[f for f in z_files if f[2:(2+len(i))]==i] for i in z_index ]\n",
    "    \n",
    "    for i in z_2d_files:\n",
    "        #z_score_df = pd.DataFrame()\n",
    "        #make zscores \n",
    "        for j in i:\n",
    "            df = get_zscore_for_one_year('./Combined_Data/'+j)\n",
    "            df = df.sort_values('GAME_DATE')\n",
    "            df.to_csv(zdir + \"/z_\" + j[2:], index=False)  \n",
    "           \n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n",
      "Inside get_zscore_for_one_year\n"
     ]
    }
   ],
   "source": [
    "x = big_system(year = \"17-20\", rolling = [3,5,10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6c9b811a7586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
