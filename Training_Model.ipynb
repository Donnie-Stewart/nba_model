{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nba_api\n",
    "\n",
    "from nba_api.stats.static import teams\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filename, start_date, end_date):\n",
    "    nba_teams = teams.get_teams()\n",
    "    # if the file does not exist, create with a csv that aggregates all raw data\n",
    "    if (path.exists(filename) != True):\n",
    "        \n",
    "        team_id = nba_teams[0]['id']\n",
    "        gamefinder = leaguegamefinder.LeagueGameFinder(date_from_nullable = start_date , date_to_nullable = end_date , team_id_nullable = team_id)\n",
    "        games = gamefinder.get_data_frames()[0]\n",
    "        games.to_csv(filename,index=False)\n",
    "    \n",
    "    # if the file exists, start with the row that the csv left off at\n",
    "    if (path.exists(filename) == True):   \n",
    "        \n",
    "        old_df = pd.read_csv(filename)\n",
    "        last_id = old_df['TEAM_ID'][len(old_df)-1]\n",
    "        start_id = int(last_id) + 1\n",
    "        \n",
    "        while start_id <= 1610612766:\n",
    "            old_df = pd.read_csv(filename)\n",
    "            gamefinder = leaguegamefinder.LeagueGameFinder(date_from_nullable = start_date , date_to_nullable = end_date , team_id_nullable = start_id)\n",
    "            games = gamefinder.get_data_frames()[0]\n",
    "            new_df = old_df.append(games)\n",
    "            new_df.to_csv(filename, index=False)\n",
    "            start_id = start_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_reb_given_up():\n",
    "    df = pd.read_csv('/Users/abhibegur/Documents/GitHub/NBA_model_Dahbi/nba_model/Raw_Data/2017-18.csv')\n",
    "    off_reb_given_up = []\n",
    "    for row in df.iterrows():\n",
    "        catch = df.loc[df['GAME_ID'] == row[1]['GAME_ID']]\n",
    "        row[1]['OREB_GIVEN_UP'] = catch.iloc[1]['OREB']\n",
    "        off_reb_given_up.append(row[1]['OREB_GIVEN_UP'])\n",
    "    df.insert(loc=(df.columns.get_loc(\"REB\")) + 1, column=\"OREB_GIVEN_UP\", value=off_reb_given_up)\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEASON_ID              int64\n",
      "TEAM_ID                int64\n",
      "TEAM_ABBREVIATION     object\n",
      "TEAM_NAME             object\n",
      "GAME_ID                int64\n",
      "GAME_DATE             object\n",
      "MATCHUP               object\n",
      "WL                    object\n",
      "MIN                    int64\n",
      "PTS                    int64\n",
      "FGM                    int64\n",
      "FGA                    int64\n",
      "FG_PCT               float64\n",
      "FG3M                   int64\n",
      "FG3A                   int64\n",
      "FG3_PCT              float64\n",
      "FTM                    int64\n",
      "FTA                    int64\n",
      "FT_PCT               float64\n",
      "OREB                   int64\n",
      "DREB                   int64\n",
      "REB                    int64\n",
      "AST                    int64\n",
      "STL                    int64\n",
      "BLK                    int64\n",
      "TOV                    int64\n",
      "PF                     int64\n",
      "PLUS_MINUS           float64\n",
      "dtype: object\n",
      "   SEASON_ID     TEAM_ID TEAM_ABBREVIATION      TEAM_NAME   GAME_ID  \\\n",
      "0      22017  1610612737               ATL  Atlanta Hawks  21700009   \n",
      "1      22017  1610612737               ATL  Atlanta Hawks  21700017   \n",
      "2      22017  1610612737               ATL  Atlanta Hawks  21700038   \n",
      "3      22017  1610612737               ATL  Atlanta Hawks  21700042   \n",
      "4      22017  1610612737               ATL  Atlanta Hawks  21700065   \n",
      "\n",
      "    GAME_DATE    MATCHUP WL  MIN  PTS  ...  OREB  DREB  REB  OREB_GIVEN_UP  \\\n",
      "0  2017-10-18  ATL @ DAL  W  241  117  ...    14    36   50              9   \n",
      "1  2017-10-20  ATL @ CHA  L  241   91  ...     6    32   38             12   \n",
      "2  2017-10-22  ATL @ BKN  L  239  104  ...    16    32   48             13   \n",
      "3  2017-10-23  ATL @ MIA  L  240   93  ...     6    35   41             10   \n",
      "4  2017-10-26  ATL @ CHI  L  240   86  ...     7    33   40             18   \n",
      "\n",
      "   AST  STL  BLK  TOV  PF  PLUS_MINUS  \n",
      "0   20   11    6   13  18         6.0  \n",
      "1   19   11    4   13  29       -22.0  \n",
      "2   21    6    2   16  25       -12.0  \n",
      "3   14    9    1   20  17       -11.0  \n",
      "4   22   10    5   10  23        -5.0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "off_reb_given_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_rest():\n",
    "    df = pd.read_csv('/Users/abhibegur/Documents/GitHub/NBA_model_Dahbi/nba_model/Raw_Data/2017-18.csv')\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_id = nba_teams[0]['id']\n",
    "    df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "\n",
    "    df['REST'] = df.groupby(['TEAM_ABBREVIATION'])['GAME_DATE'].diff()\n",
    "    print(df)\n",
    "#     while team_id <= 1610612766:\n",
    "        \n",
    "#         team_id = team_id + 1\n",
    "#     print(team_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SEASON_ID     TEAM_ID TEAM_ABBREVIATION           TEAM_NAME   GAME_ID  \\\n",
      "0         22017  1610612737               ATL       Atlanta Hawks  21700009   \n",
      "1         22017  1610612737               ATL       Atlanta Hawks  21700017   \n",
      "2         22017  1610612737               ATL       Atlanta Hawks  21700038   \n",
      "3         22017  1610612737               ATL       Atlanta Hawks  21700042   \n",
      "4         22017  1610612737               ATL       Atlanta Hawks  21700065   \n",
      "...         ...         ...               ...                 ...       ...   \n",
      "2619      42017  1610612764               WAS  Washington Wizards  41700102   \n",
      "2620      42017  1610612764               WAS  Washington Wizards  41700103   \n",
      "2621      42017  1610612764               WAS  Washington Wizards  41700104   \n",
      "2622      42017  1610612764               WAS  Washington Wizards  41700105   \n",
      "2623      42017  1610612764               WAS  Washington Wizards  41700106   \n",
      "\n",
      "      GAME_DATE      MATCHUP WL  MIN  PTS  ...  OREB  DREB  REB  AST  STL  \\\n",
      "0    2017-10-18    ATL @ DAL  W  241  117  ...    14    36   50   20   11   \n",
      "1    2017-10-20    ATL @ CHA  L  241   91  ...     6    32   38   19   11   \n",
      "2    2017-10-22    ATL @ BKN  L  239  104  ...    16    32   48   21    6   \n",
      "3    2017-10-23    ATL @ MIA  L  240   93  ...     6    35   41   14    9   \n",
      "4    2017-10-26    ATL @ CHI  L  240   86  ...     7    33   40   22   10   \n",
      "...         ...          ... ..  ...  ...  ...   ...   ...  ...  ...  ...   \n",
      "2619 2018-04-17    WAS @ TOR  L  240  119  ...     4    30   34   26    7   \n",
      "2620 2018-04-20  WAS vs. TOR  W  241  122  ...     7    32   39   29   10   \n",
      "2621 2018-04-22  WAS vs. TOR  W  242  106  ...     9    31   40   23   10   \n",
      "2622 2018-04-25    WAS @ TOR  L  240   98  ...    14    36   50   21    6   \n",
      "2623 2018-04-27  WAS vs. TOR  L  239   92  ...    11    31   42   12    3   \n",
      "\n",
      "      BLK  TOV  PF  PLUS_MINUS   REST  \n",
      "0       6   13  18         6.0    NaT  \n",
      "1       4   13  29       -22.0 2 days  \n",
      "2       2   16  25       -12.0 2 days  \n",
      "3       1   20  17       -11.0 1 days  \n",
      "4       5   10  23        -5.0 3 days  \n",
      "...   ...  ...  ..         ...    ...  \n",
      "2619    3    8  24       -11.0 3 days  \n",
      "2620    8   11  21        19.0 3 days  \n",
      "2621    8   13  23         8.0 2 days  \n",
      "2622    4   15  18       -10.0 3 days  \n",
      "2623    5   14  19       -10.0 2 days  \n",
      "\n",
      "[2624 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "days_rest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_handle(count, filename, start_date, end_date):\n",
    "    try:\n",
    "        print(\"-----try is running-----\")\n",
    "        # put csv name here\n",
    "        getData(filename, start_date, end_date)\n",
    "        count = 0\n",
    "        \n",
    "    except:\n",
    "        if count < 25:\n",
    "            print(\"-----exception handled-----\", count)\n",
    "            error_handle(count + 1,filename, start_date, end_date)\n",
    "        else:\n",
    "            print(\"-----max tries exceeded-----\")\n",
    "    \n",
    "    nba_teams = teams.get_teams()\n",
    "    csv_df = pd.read_csv(filename)\n",
    "    cdf = csv_df.sort_values(['TEAM_ABBREVIATION','GAME_DATE'] , ascending=[True, True])\n",
    "    cdf.to_csv(filename, index=False)\n",
    "    \n",
    "    #return rolling_average_stats(filename, 'ten_day-' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of csv to read, name of csv to write\n",
    "def rolling_average_stats(r_filename, w_filename, i):  \n",
    "    print('Inside rolling_average_stats')\n",
    "    \n",
    "    nba_teams = teams.get_teams()\n",
    "    csv_df = pd.read_csv(r_filename)\n",
    "\n",
    "    list_points = []\n",
    "    list_team_points = []\n",
    "    x = 1\n",
    "    # for each team in csv, calculate the rolling average based on the parameter, i, that is passed in\n",
    "    for team in nba_teams:\n",
    "        team_df = csv_df[csv_df['TEAM_ID'] == team['id']]\n",
    "        for col in team_df.columns[9:]:\n",
    "            team_df['AV_'+ col] = team_df[col].rolling(window=i).mean()\n",
    "            team_df['AV_'+ col] = team_df['AV_'+ col].shift(1)\n",
    "        head = list(team_df.columns.values)\n",
    "        if x == 1:\n",
    "            team_df.to_csv(w_filename, header=head, index=False)\n",
    "            x = x+1\n",
    "\n",
    "        else:\n",
    "            team_df.to_csv(w_filename, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_team_games(df, keep_method='home'):\n",
    "    '''Combine a TEAM_ID-GAME_ID unique table into rows by game. Slow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : Input DataFrame.\n",
    "        keep_method : {'home', 'away', 'winner', 'loser', ``None``}, default 'home'\n",
    "            - 'home' : Keep rows where TEAM_A is the home team.\n",
    "            - 'away' : Keep rows where TEAM_A is the away team.\n",
    "            - 'winner' : Keep rows where TEAM_A is the losing team.\n",
    "            - 'loser' : Keep rows where TEAM_A is the winning team.\n",
    "            - ``None`` : Keep all rows. Will result in an output DataFrame the same\n",
    "                length as the input DataFrame.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        result : DataFrame\n",
    "    '''\n",
    "    # Join every row to all others with the same game ID.\n",
    "    joined = pd.merge(df, df, suffixes=['_A', '_B'],\n",
    "                      on=['SEASON_ID', 'GAME_ID', 'GAME_DATE'])\n",
    "    # Filter out any row that is joined to itself.\n",
    "    result = joined[joined.TEAM_ID_A != joined.TEAM_ID_B]\n",
    "    # Take action based on the keep_method flag.\n",
    "    if keep_method is None:\n",
    "        # Return all the rows.\n",
    "        pass\n",
    "    elif keep_method.lower() == 'home':\n",
    "        # Keep rows where TEAM_A is the home team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' vs. ')]\n",
    "    elif keep_method.lower() == 'away':\n",
    "        # Keep rows where TEAM_A is the away team.\n",
    "        result = result[result.MATCHUP_A.str.contains(' @ ')]\n",
    "    elif keep_method.lower() == 'winner':\n",
    "        result = result[result.WL_A == 'W']\n",
    "    elif keep_method.lower() == 'loser':\n",
    "        result = result[result.WL_A == 'L']\n",
    "    else:\n",
    "        raise ValueError(f'Invalid keep_method: {keep_method}')\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses combine function and cleans csv\n",
    "# ten_day_csv is the csv with the rolling ten day averages for a year\n",
    "# combined_csv is the returned csv with teams combined with their matchups\n",
    "def combine_and_clean(ten_day_csv, combined_csv):\n",
    "    print('Inside combine_and_clean')\n",
    "    \n",
    "    attempt = pd.read_csv(ten_day_csv,index_col=[0])\n",
    "    # Drop these fields to only leave averages\n",
    "    attempt = attempt.drop(['PTS','FGM','FGA','FG_PCT','FG3M','FG3A','FG3_PCT','FTM','FTA','FT_PCT','OREB','DREB','REB','AST','STL','BLK','TOV','PF', 'PLUS_MINUS'],axis=1)\n",
    "\n",
    "    count = 0\n",
    "    # combine rows from rolling average pdf so that it illustrates TEAM_A attributes vs TEAM_B attributes (side by side)\n",
    "    for row in attempt.iterrows():\n",
    "        # creates csv\n",
    "        if (count == 0):\n",
    "            catch = attempt.loc[attempt['GAME_ID'] == row[1]['GAME_ID']]\n",
    "            catch = pd.DataFrame(catch)\n",
    "            combine = combine_team_games(catch)\n",
    "            combine.to_csv(combined_csv, index=False)\n",
    "            count = count + 1\n",
    "        # appends csv\n",
    "        else: \n",
    "            old_df = pd.read_csv(combined_csv)\n",
    "            catch = attempt.loc[attempt['GAME_ID'] == row[1]['GAME_ID']]\n",
    "            catch = pd.DataFrame(catch)\n",
    "            combine = combine_team_games(catch)\n",
    "            new_df = old_df.append(combine)\n",
    "            new_df.to_csv(combined_csv, index=False)\n",
    "    \n",
    "    clean = pd.read_csv(combined_csv)\n",
    "    # drops duplicates, sort by game date, and replace W with 1 and L with 0\n",
    "    cleaned = clean.drop_duplicates(subset='GAME_ID')\n",
    "    cleaned = cleaned.sort_values('GAME_DATE')\n",
    "    cleaned['WL_A'] = cleaned['WL_A'].replace(['W','L'],[1,0])\n",
    "    cleaned['WL_B'] = cleaned['WL_B'].replace(['W','L'],[1,0])\n",
    "    cleaned.to_csv(combined_csv, index=False)\n",
    "    return combined_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscore_for_one_year(cleaned_csv):\n",
    "    #print(\"Inside get_zscore_for_one_year\")\n",
    "    \n",
    "    data = pd.read_csv(cleaned_csv)\n",
    "    data = data.dropna()\n",
    "\n",
    "    z_data = pd.DataFrame(columns = ['GAME_ID', 'GAME_DATE', 'MATCHUP','WL', 'PTS', 'FGM' , 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PLUS_MINUS'])\n",
    "    z_data['WL'] = data['WL_A']\n",
    "    z_data['GAME_ID'] = data['GAME_ID']\n",
    "    z_data['GAME_DATE'] = data['GAME_DATE']\n",
    "    z_data['MATCHUP'] = data['MATCHUP_A']\n",
    "    # for each of the rolling averages that were calculated, take the difference of TEAM_A's averages - TEAM_B's averages\n",
    "    for column in z_data.columns[4:]:\n",
    "        z_data[column] = data['AV_' + column + '_A'] - data['AV_' + column + '_B']\n",
    "    # Formatting\n",
    "    z_data = z_data.dropna()\n",
    "    z_data = z_data.round(decimals=3)\n",
    "    return z_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zscores(year1_cleaned_csv, year2_cleaned_csv, year3_cleaned_csv):\n",
    "    \n",
    "    df1 = get_zscore_for_one_year(year1_cleaned_csv)\n",
    "    df2 = get_zscore_for_one_year(year2_cleaned_csv)\n",
    "    df3 = get_zscore_for_one_year(year3_cleaned_csv)\n",
    "    \n",
    "    df1 = df1.append(df2)\n",
    "    df1 = df1.append(df3)\n",
    "    df1.to_csv(\"all_zscores.csv\", index=False)\n",
    "    return performLogReg(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the logistic regression model and tests accuracy\n",
    "def performLogReg(dataframe):\n",
    "\n",
    "    # Update if new stats are added\n",
    "    featureColumns = ['PTS', 'FGM', 'FGA', 'FG3_PCT', 'FTA','REB', 'AST',  'STL', 'TOV']\n",
    "\n",
    "    X = dataframe[featureColumns] # Features\n",
    "    Y = dataframe['WL'] # Target Variable\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=True)\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    logreg.fit(X_train, Y_train)  # Fits model with data\n",
    "    filename = 'finalized_model.sav'\n",
    "    pickle.dump(logreg, open(filename, 'wb'))\n",
    "\n",
    "    Y_pred = logreg.predict(X_test)\n",
    "\n",
    "    confusionMatrix = metrics.confusion_matrix(Y_test, Y_pred)  # Diagonals tell you correct predictions\n",
    "\n",
    "    # Code below prints model accuracy information\n",
    "    print('Coefficient Information:')\n",
    "\n",
    "    for i in range(len(featureColumns)):  # Prints each feature next to its corresponding coefficient in the model\n",
    "\n",
    "        logregCoefficients = logreg.coef_\n",
    "\n",
    "        currentFeature = featureColumns[i]\n",
    "        currentCoefficient = logregCoefficients[0][i]\n",
    "\n",
    "        print(currentFeature + ': ' + str(currentCoefficient))\n",
    "\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(Y_test, Y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(Y_test, Y_pred))\n",
    "\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusionMatrix)\n",
    "\n",
    "    return logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "    \n",
    "#     logreg = get_zscores(error_handle(0, \"2017-18.csv\", \"10/17/2017\", \"06/17/2018\"), error_handle(0, \"2018-19.csv\", \"10/16/2018\", \"06/13/2019\"), error_handle(0, \"2019-20.csv\", \"10/22/2019\", \"10/11/2020\"))\n",
    "#     print(logreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_date_dict = {\n",
    "    \"17-18\" : [\"10/17/2017\", \"06/17/2018\"],\n",
    "    \"18-19\" : [\"10/16/2018\", \"06/13/2019\"],\n",
    "    \"19-20\" : [\"10/22/2019\", \"10/11/2020\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-ea7d7576008c>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-ea7d7576008c>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    combine and clean team data\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def big_system(year = \"19-20\", rolling = [10]):\n",
    "    \"\"\" \n",
    "    params:\n",
    "    year | specify season e.g. \"19-20\" or range e.g. \"17-20\"\n",
    "    rolling | specify amount of days as list e.g. \"[3,10] or [10]\"\n",
    "    \"\"\"\n",
    "    #get raw data for years specified\n",
    "    range_years = year.split(\"-\")\n",
    "    if (int(range_years[0]) - int(range_years[1])) != 1:\n",
    "        x = range_years[0]\n",
    "        years_list = []\n",
    "        while int(x) < int(range_years[1]):\n",
    "            y = int(x)+1\n",
    "            years_list.append(x + \"-\" + str(y))\n",
    "            x = str(y)\n",
    "        outdir = './Raw_Data'\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        for y in years_list:\n",
    "            fname = \"20\" + y + \".csv\"\n",
    "            fullname = os.path.join(outdir, fname)  \n",
    "            error_handle(0, fullname, season_date_dict[y][0], season_date_dict[y][1])\n",
    "    else:\n",
    "        outdir = './Raw_Data'\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        fname = \"20\" + year + \".csv\"\n",
    "        fullname = os.path.join(outdir, fname)  \n",
    "        error_handle(0, fullname, season_date_dict[year][0], season_date_dict[year][1])\n",
    "\n",
    "    #adjust rolling average deadline\n",
    "    if type(rolling == list):\n",
    "        xdir = './Rolling_Averages'\n",
    "        if not os.path.exists(xdir):\n",
    "            os.mkdir(xdir)\n",
    "        files = [ f for f in listdir('./Raw_Data') if isfile(join('./Raw_Data', f))]      \n",
    "\n",
    "        for f in files:            \n",
    "            for i in rolling:\n",
    "                wname =  str(i)+ \"_rolling_\" + f\n",
    "                w_filename = os.path.join(xdir, wname) \n",
    "                rolling_average_stats('./Raw_Data/' + f, w_filename, i)\n",
    "                \n",
    "#     combine and clean team data\n",
    "    cdir = './Combined_Data'\n",
    "    if not os.path.exists(cdir):\n",
    "        os.mkdir(cdir)\n",
    "    r_files = [ f for f in listdir('./Rolling_Averages') if isfile(join('./Rolling_Averages', f))]      \n",
    "    for f in r_files:\n",
    "        combine_and_clean('./Rolling_Averages/'+ f, './Combined_Data/'+'c_' + f )\n",
    "\n",
    "    #zscore and finalize\n",
    "    zdir = './Zscores'\n",
    "    if not os.path.exists(zdir):\n",
    "        os.mkdir(zdir)\n",
    "        \n",
    "    z_files = [ f for f in listdir('./Combined_Data') if isfile(join('./Combined_Data', f))]\n",
    "   \n",
    "    z_index = []\n",
    "    for f in z_files:\n",
    "        split = f.split('_')\n",
    "        i = (split[1])\n",
    "        if i not in z_index:\n",
    "            z_index.append(i)\n",
    "    z_2d_files = [[f for f in z_files if f[2:(2+len(i))]==i] for i in z_index ]\n",
    "    \n",
    "    for i in z_2d_files:\n",
    "        #z_score_df = pd.DataFrame()\n",
    "        #make zscores \n",
    "        for j in i:\n",
    "            df = get_zscore_for_one_year('./Combined_Data/'+j)\n",
    "            df = df.sort_values('GAME_DATE')\n",
    "            df.to_csv(zdir + \"/z_\" + j[2:], index=False)  \n",
    "           \n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----try is running-----\n",
      "-----try is running-----\n",
      "-----try is running-----\n",
      "Inside rolling_average_stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhibegur/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/abhibegur/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside rolling_average_stats\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n",
      "Inside combine_and_clean\n"
     ]
    }
   ],
   "source": [
    "x = big_system(year = \"17-20\", rolling = [3,5,10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6c9b811a7586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
